# RESULTS

## CoNLL-2014

<details>
  <summary><b>CLAUDE-SONNET-4-5</b></summary>

Neutral:

```bash
python eval.py m2scorer "results/conll2014/claude-sonnet-4-5/claude-sonnet-4-5_official-2014.combined-withalt.m2_zs-neutral.txt_202512311557.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/claude-sonnet-4-5/claude-sonnet-4-5_official-2014.combined-withalt.m2_zs-neutral.txt_202512311557.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6458
Recall      : 0.6171
F_0.5       : 0.6398
```

Min-Edit:

```bash
python eval.py m2scorer "results/conll2014/claude-sonnet-4-5/claude-sonnet-4-5_official-2014.combined-withalt.m2_zs-minedit.txt_202512310457.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/claude-sonnet-4-5/claude-sonnet-4-5_official-2014.combined-withalt.m2_zs-minedit.txt_202512310457.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6914
Recall      : 0.5982
F_0.5       : 0.6705
```
  
</details>

<details>
  <summary><b>GPT-3.5-TURBO</b></summary>

Neutral:

```bash
python eval.py m2scorer "results/conll2014/gpt-3.5-turbo/gpt-3.5-turbo_official-2014.combined-withalt.m2_zs-neutral.txt_202512310300.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/gpt-3.5-turbo/gpt-3.5-turbo_official-2014.combined-withalt.m2_zs-neutral.txt_202512310300.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6505
Recall      : 0.5396
F_0.5       : 0.6248
```

Min-Edit:

```bash
python eval.py m2scorer "results/conll2014/gpt-3.5-turbo/gpt-3.5-turbo_official-2014.combined-withalt.m2_zs-minedit.txt_202512310228.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/gpt-3.5-turbo/gpt-3.5-turbo_official-2014.combined-withalt.m2_zs-minedit.txt_202512310228.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6593
Recall      : 0.5365
F_0.5       : 0.6305
```
  
</details>

<details>
  <summary><b>GPT-4.1-2025-04-14</b></summary>

Neutral:

```bash
python eval.py m2scorer "results/conll2014/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_official-2014.combined-withalt.m2_zs-neutral.txt_202512311403.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_official-2014.combined-withalt.m2_zs-neutral.txt_202512311403.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.5726
Recall      : 0.6584
F_0.5       : 0.5879
```

Min-Edit:

```bash
python eval.py m2scorer "results/conll2014/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_official-2014.combined-withalt.m2_zs-minedit.txt_202512311437.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_official-2014.combined-withalt.m2_zs-minedit.txt_202512311437.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.7037
Recall      : 0.5273
F_0.5       : 0.6596
```
  
</details>

<details>
  <summary><b>GPT-5-MINI-2025-08-07</b></summary>

Neutral:

```bash
python eval.py m2scorer "results/conll2014/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_official-2014.combined-withalt.m2_zs-neutral.txt_202601020040.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_official-2014.combined-withalt.m2_zs-neutral.txt_202601020040.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.4433
Recall      : 0.6655
F_0.5       : 0.4751
```

Min-Edit:

```bash
python eval.py m2scorer "results/conll2014/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_official-2014.combined-withalt.m2_zs-minedit.txt_202601050119.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_official-2014.combined-withalt.m2_zs-minedit.txt_202601050119.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6252
Recall      : 0.6134
F_0.5       : 0.6228
```
  
</details>

<details>
  <summary><b>LLAMA-3.3-70B-INSTRUCT</b></summary>

Neutral:

```bash
python eval.py m2scorer "results/conll2014/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_official-2014.combined-withalt.m2_zs-neutral.txt_202601031741.txt" "data/conll2014/alt/official-2014.combined-withalt.m2"
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_official-2014.combined-withalt.m2_zs-neutral.txt_202601031741.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.5945
Recall      : 0.6018
F_0.5       : 0.5960
```

Min-Edit:

```bash
python eval.py m2scorer "results/conll2014/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_official-2014.combined-withalt.m2_zs-minedit.txt_202601040237.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_official-2014.combined-withalt.m2_zs-minedit.txt_202601040237.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6712
Recall      : 0.5399
F_0.5       : 0.6401
```
  
</details>

<details>
  <summary><b>LLAMA-4-SCOUT-17B-16E-INSTRUCT</b></summary>

Neutral:

```bash
python eval.py m2scorer "results/conll2014/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_official-2014.combined-withalt.m2_zs-neutral.txt_202601050241.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_official-2014.combined-withalt.m2_zs-neutral.txt_202601050241.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.5812
Recall      : 0.6078
F_0.5       : 0.5863
```

Min-Edit:

```bash
python eval.py m2scorer "results/conll2014/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_official-2014.combined-withalt.m2_zs-minedit.txt_202601040233.txt" "data/conll2014/alt/official-2014.combined-withalt.m2"
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_official-2014.combined-withalt.m2_zs-minedit.txt_202601040233.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6306
Recall      : 0.5817
F_0.5       : 0.6202
```
  
</details>

<details>
  <summary><b>MISTRAL-LARGE-3</b></summary>

Neutral:

```bash
python eval.py m2scorer "results/conll2014/Mistral-Large-3/Mistral-Large-3_official-2014.combined-withalt.m2_zs-neutral.txt_202601031737.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/Mistral-Large-3/Mistral-Large-3_official-2014.combined-withalt.m2_zs-neutral.txt_202601031737.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.4964
Recall      : 0.6406
F_0.5       : 0.5198
```

Min-Edit:

```bash
python eval.py m2scorer "results/conll2014/Mistral-Large-3/Mistral-Large-3_official-2014.combined-withalt.m2_zs-minedit.txt_202601040233.txt" "data/conll2014/alt/official-2014.combined-withalt.m2" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : m2scorer
System Output Path      : results/conll2014/Mistral-Large-3/Mistral-Large-3_official-2014.combined-withalt.m2_zs-minedit.txt_202601040233.txt
Gold Data Path          : data/conll2014/alt/official-2014.combined-withalt.m2
############################################
Precision   : 0.6015
Recall      : 0.6025
F_0.5       : 0.6017
```

</details>

## BEA-2019

<details>
  <summary><b>CLAUDE-SONNET-4-5</b></summary>

Neutral:

```bash

```

Min-Edit:

```bash

```
  
</details>

<details>
  <summary><b>GPT-3.5-TURBO</b></summary>

Neutral:

```bash

```

Min-Edit:

```bash

```
  
</details>

<details>
  <summary><b>GPT-4.1-2025-04-14</b></summary>

Neutral:

```bash

```

Min-Edit:

```bash

```
  
</details>

<details>
  <summary><b>GPT-5-MINI-2025-08-07</b></summary>

Neutral:

```bash

```

Min-Edit:

```bash

```
  
</details>

<details>
  <summary><b>LLAMA-3.3-70B-INSTRUCT</b></summary>

Neutral:

```bash

```

Min-Edit:

```bash

```
  
</details>

<details>
  <summary><b>LLAMA-4-SCOUT-17B-16E-INSTRUCT</b></summary>

Neutral:

```bash

```

Min-Edit:

```bash

```
  
</details>

<details>
  <summary><b>MISTRAL-LARGE-3</b></summary>

Neutral:

```bash

```

Min-Edit:

```bash

```
  
</details>

## JFLEG

<details>
  <summary><b>CLAUDE-SONNET-4-5</b></summary>

Neutral:

```bash
python eval.py gleu "results/jfleg/claude-sonnet-4-5/claude-sonnet-4-5_test.src_zs-neutral.txt_202601012253.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/claude-sonnet-4-5/claude-sonnet-4-5_test.src_zs-neutral.txt_202601012253.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/claude-sonnet-4-5/claude-sonnet-4-5_test.src_zs-neutral.txt_202601012253.txt
[['0.660863', '0.006750', '(0.648,0.674)']]
```

Fluency:

```bash
python eval.py gleu "results/jfleg/claude-sonnet-4-5/claude-sonnet-4-5_test.src_zs-fluency.txt_202601051332.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/claude-sonnet-4-5/claude-sonnet-4-5_test.src_zs-fluency.txt_202601051332.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/claude-sonnet-4-5/claude-sonnet-4-5_test.src_zs-fluency.txt_202601051332.txt
[['0.638196', '0.005767', '(0.627,0.649)']]
```
  
</details>

<details>
  <summary><b>GPT-3.5-TURBO</b></summary>

Neutral:

```bash
python eval.py gleu "results/jfleg/gpt-3.5-turbo/gpt-3.5-turbo_test.src_zs-neutral.txt_202601012149.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/gpt-3.5-turbo/gpt-3.5-turbo_test.src_zs-neutral.txt_202601012149.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/gpt-3.5-turbo/gpt-3.5-turbo_test.src_zs-neutral.txt_202601012149.txt
[['0.646657', '0.007530', '(0.632,0.661)']]
```

Fluency:

```bash
python eval.py gleu "results/jfleg/gpt-3.5-turbo/gpt-3.5-turbo_test.src_zs-fluency.txt_202601042313.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/gpt-3.5-turbo/gpt-3.5-turbo_test.src_zs-fluency.txt_202601042313.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/gpt-3.5-turbo/gpt-3.5-turbo_test.src_zs-fluency.txt_202601042313.txt
[['0.639898', '0.006617', '(0.627,0.653)']]
```
  
</details>

<details>
  <summary><b>GPT-4.1-2025-04-14</b></summary>

Neutral:

```bash
python eval.py gleu "results/jfleg/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_test.src_zs-neutral.txt_202512311713.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_test.src_zs-neutral.txt_202512311713.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_test.src_zs-neutral.txt_202512311713.txt
[['0.653402', '0.006450', '(0.641,0.666)']]
```

Fluency:

```bash
python eval.py gleu "results/jfleg/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_test.src_zs-fluency.txt_202601042314.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_test.src_zs-fluency.txt_202601042314.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/gpt-4.1-2025-04-14/gpt-4.1-2025-04-14_test.src_zs-fluency.txt_202601042314.txt
[['0.603658', '0.005224', '(0.593,0.614)']]
```
  
</details>

<details>
  <summary><b>GPT-5-MINI-2025-08-07</b></summary>

Neutral:

```bash
python eval.py gleu "results/jfleg/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_test.src_zs-neutral.txt_202601072237.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_test.src_zs-neutral.txt_202601072237.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_test.src_zs-neutral.txt_202601072237.txt
[['0.551201', '0.004758', '(0.542,0.561)']]
```

Fluency:

```bash
python eval.py gleu "results/jfleg/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_test.src_zs-fluency.txt_202601071324.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_test.src_zs-fluency.txt_202601071324.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/gpt-5-mini-2025-08-07/gpt-5-mini-2025-08-07_test.src_zs-fluency.txt_202601071324.txt
[['0.480817', '0.004105', '(0.473,0.489)']]
```
  
</details>

<details>
  <summary><b>LLAMA-3.3-70B-INSTRUCT</b></summary>

Neutral:

```bash
python eval.py gleu "results/jfleg/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_test.src_zs-neutral.txt_202601070003.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_test.src_zs-neutral.txt_202601070003.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_test.src_zs-neutral.txt_202601070003.txt
[['0.636791', '0.006864', '(0.623,0.650)']]
```

Fluency:

```bash
python eval.py gleu "results/jfleg/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_test.src_zs-fluency.txt_202601071409.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_test.src_zs-fluency.txt_202601071409.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/Llama-3.3-70B-Instruct/Llama-3.3-70B-Instruct_test.src_zs-fluency.txt_202601071409.txt
[['0.620391', '0.006145', '(0.608,0.632)']]
```
  
</details>

<details>
  <summary><b>LLAMA-4-SCOUT-17B-16E-INSTRUCT</b></summary>

Neutral:

```bash
python eval.py gleu "results/jfleg/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_test.src_zs-neutral.txt_202601072334.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_test.src_zs-neutral.txt_202601072334.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_test.src_zs-neutral.txt_202601072334.txt
[['0.611061', '0.006215', '(0.599,0.623)']]
```

Fluency:

```bash
python eval.py gleu "results/jfleg/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_test.src_zs-fluency.txt_202601071416.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_test.src_zs-fluency.txt_202601071416.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/Llama-4-Scout-17B-16E-Instruct/Llama-4-Scout-17B-16E-Instruct_test.src_zs-fluency.txt_202601071416.txt
[['0.587022', '0.005603', '(0.576,0.598)']]
```
  
</details>

<details>
  <summary><b>MISTRAL-LARGE-3</b></summary>

Neutral:

```bash
python eval.py gleu "results/jfleg/Mistral-Large-3/Mistral-Large-3_test.src_zs-neutral.txt_202601070009.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/Mistral-Large-3/Mistral-Large-3_test.src_zs-neutral.txt_202601070009.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/Mistral-Large-3/Mistral-Large-3_test.src_zs-neutral.txt_202601070009.txt
[['0.598847', '0.005752', '(0.588,0.610)']]
```

Fluency:

```bash
python eval.py gleu "results/jfleg/Mistral-Large-3/Mistral-Large-3_test.src_zs-fluency.txt_202601071418.txt" "./data/jfleg/test" 
✓ All CLI arguments validated successfully.
############################################
Metric                  : gleu
System Output Path      : results/jfleg/Mistral-Large-3/Mistral-Large-3_test.src_zs-fluency.txt_202601071418.txt
Gold Data Path          : ./data/jfleg/test
############################################
Running GLEU...
results/jfleg/Mistral-Large-3/Mistral-Large-3_test.src_zs-fluency.txt_202601071418.txt
[['0.500558', '0.004377', '(0.492,0.509)']]
```
  
</details>